<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content='width=device-width,initial-scale=1'><meta name=description content="How to fix the rolling deployment timeout issue ?"><title>The Status Order Fail During Rolling Deployments - PART2</title><link rel=canonical href=https://quanghugo.pages.dev/p/rolling-production-p2/><link rel=stylesheet href=/scss/style.min.ac862b5e206eadebeaa833dbebb8fe395bb5fb68e93f43adcb4539a45a6a60da.css><meta property='og:title' content="The Status Order Fail During Rolling Deployments - PART2"><meta property='og:description' content="How to fix the rolling deployment timeout issue ?"><meta property='og:url' content='https://quanghugo.pages.dev/p/rolling-production-p2/'><meta property='og:site_name' content='Quang Hugo'><meta property='og:type' content='article'><meta property='article:section' content='Post'><meta property='article:published_time' content='2026-01-10T02:30:00+00:00'><meta property='article:modified_time' content='2026-01-10T02:30:00+00:00'><meta name=twitter:title content="The Status Order Fail During Rolling Deployments - PART2"><meta name=twitter:description content="How to fix the rolling deployment timeout issue ?"><link rel="shortcut icon" href=/logo.png></head><body class=article-page><script>(function(){const e="StackColorScheme";localStorage.getItem(e)||localStorage.setItem(e,"auto")})()</script><script>(function(){const t="StackColorScheme",e=localStorage.getItem(t),n=window.matchMedia("(prefers-color-scheme: dark)").matches===!0;e=="dark"||e==="auto"&&n?document.documentElement.dataset.scheme="dark":document.documentElement.dataset.scheme="light"})()</script><div class="container main-container flex on-phone--column extended"><aside class="sidebar left-sidebar sticky"><button class="hamburger hamburger--spin" type=button id=toggle-menu aria-label="Toggle Menu">
<span class=hamburger-box><span class=hamburger-inner></span></span></button><header><figure class=site-avatar><a href=/><img src=/img/avatar.jpg width=300 height=300 class=site-logo loading=lazy alt=Avatar>
</a><span class=emoji>üê≥</span></figure><div class=site-meta><h1 class=site-name><a href=/>Quang Hugo</a></h1><h2 class=site-description>Living with passion is extremely important for a fulfilling life!</h2></div></header><ol class=menu-social><li><a href=https://github.com/qnit18 target=_blank title=GitHub rel=me><svg class="icon icon-tabler icon-tabler-brand-github" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M9 19c-4.3 1.4-4.3-2.5-6-3m12 5v-3.5c0-1 .1-1.4-.5-2 2.8-.3 5.5-1.4 5.5-6a4.6 4.6.0 00-1.3-3.2 4.2 4.2.0 00-.1-3.2s-1.1-.3-3.5 1.3a12.3 12.3.0 00-6.2.0C6.5 2.8 5.4 3.1 5.4 3.1a4.2 4.2.0 00-.1 3.2A4.6 4.6.0 004 9.5c0 4.6 2.7 5.7 5.5 6-.6.6-.6 1.2-.5 2V21"/></svg></a></li><li><a href=https://youtube.com target=_blank title=Youtube rel=me><svg class="icon icon-tabler icon-tabler-brand-youtube" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z" fill="none"/><path d="M2 8a4 4 0 014-4h12a4 4 0 014 4v8a4 4 0 01-4 4H6a4 4 0 01-4-4V8z"/><path d="M10 9l5 3-5 3z"/></svg></a></li></ol><ol class=menu id=main-menu><li><a href=/><svg class="icon icon-tabler icon-tabler-home" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><polyline points="5 12 3 12 12 3 21 12 19 12"/><path d="M5 12v7a2 2 0 002 2h10a2 2 0 002-2v-7"/><path d="M9 21v-6a2 2 0 012-2h2a2 2 0 012 2v6"/></svg>
<span>Home</span></a></li><li><a href=/archives/><svg class="icon icon-tabler icon-tabler-archive" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><rect x="3" y="4" width="18" height="4" rx="2"/><path d="M5 8v10a2 2 0 002 2h10a2 2 0 002-2V8"/><line x1="10" y1="12" x2="14" y2="12"/></svg>
<span>Archives</span></a></li><li><a href=/search/><svg class="icon icon-tabler icon-tabler-search" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="10" cy="10" r="7"/><line x1="21" y1="21" x2="15" y2="15"/></svg>
<span>Search</span></a></li><li><a href=/links/><svg class="icon icon-tabler icon-tabler-link" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M10 14a3.5 3.5.0 005 0l4-4a3.5 3.5.0 00-5-5l-.5.5"/><path d="M14 10a3.5 3.5.0 00-5 0l-4 4a3.5 3.5.0 005 5l.5-.5"/></svg>
<span>Links</span></a></li><li class=menu-bottom-section><ol class=menu><li id=dark-mode-toggle><svg class="icon icon-tabler icon-tabler-toggle-left" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="8" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<svg class="icon icon-tabler icon-tabler-toggle-right" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="16" cy="12" r="2"/><rect x="2" y="6" width="20" height="12" rx="6"/></svg>
<span>Dark Mode</span></li></ol></li></ol></aside><aside class="sidebar right-sidebar sticky"><section class="widget archives"><div class=widget-icon><svg class="icon icon-tabler icon-tabler-hash" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><line x1="5" y1="9" x2="19" y2="9"/><line x1="5" y1="15" x2="19" y2="15"/><line x1="11" y1="4" x2="7" y2="20"/><line x1="17" y1="4" x2="13" y2="20"/></svg></div><h2 class="widget-title section-title">Table of contents</h2><div class=widget--toc><nav id=TableOfContents><ol><li><a href=#the-problem-quick-recap>The Problem (Quick Recap)</a></li><li><a href=#the-core-insight>The Core Insight</a><ol><li><a href=#the-calendar-analogy>The Calendar Analogy</a></li></ol></li><li><a href=#the-solution-three-key-components>The Solution: Three Key Components</a><ol><li><a href=#1-the-shared-timeout-list>1. The Shared Timeout List</a></li><li><a href=#2-the-worker-pattern>2. The Worker Pattern</a></li><li><a href=#3-the-lock-preventing-duplicates>3. The Lock (Preventing Duplicates)</a></li></ol></li><li><a href=#how-it-all-works-together>How It All Works Together</a></li><li><a href=#what-happens-during-deployment>What Happens During Deployment</a><ol><li><a href=#old-system-in-memory-timer>Old System: In-Memory Timer</a></li><li><a href=#new-system-shared-timeout-list>New System: Shared Timeout List</a></li></ol></li><li><a href=#trade-offs-and-considerations>Trade-offs and Considerations</a><ol><li><a href=#-benefits>‚úÖ Benefits</a></li><li><a href=#-trade-offs>‚öñÔ∏è Trade-offs</a></li></ol></li><li><a href=#real-world-impact>Real-World Impact</a></li><li><a href=#key-takeaways>Key Takeaways</a><ol><li><a href=#1-persistent-state-survives-restarts>1. Persistent State Survives Restarts</a></li><li><a href=#2-shared-storage-enables-distributed-work>2. Shared Storage Enables Distributed Work</a></li><li><a href=#3-locks-prevent-duplicate-processing>3. Locks Prevent Duplicate Processing</a></li><li><a href=#4-small-delays-are-often-acceptable>4. Small Delays Are Often Acceptable</a></li><li><a href=#5-idempotency-handles-edge-cases>5. Idempotency Handles Edge Cases</a></li></ol></li><li><a href=#conclusion>Conclusion</a></li></ol></nav></div></section></aside><main class="main full-width"><article class=main-article><header class=article-header><div class=article-details><header class=article-category><a href=/categories/design-system/>Design System
</a><a href=/categories/production-issues/>Production Issues
</a><a href=/categories/distributed-systems/>Distributed Systems
</a><a href=/categories/redis/>Redis</a></header><div class=article-title-wrapper><h2 class=article-title><a href=/p/rolling-production-p2/>The Status Order Fail During Rolling Deployments - PART2</a></h2><h3 class=article-subtitle>How to fix the rolling deployment timeout issue ?</h3></div><footer class=article-time><div><svg class="icon icon-tabler icon-tabler-calendar-time" width="56" height="56" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><path d="M11.795 21H5a2 2 0 01-2-2V7a2 2 0 012-2h12a2 2 0 012 2v4"/><circle cx="18" cy="18" r="4"/><path d="M15 3v4"/><path d="M7 3v4"/><path d="M3 11h16"/><path d="M18 16.496V18l1 1"/></svg>
<time class=article-time--published datetime=2026-01-10T02:30:00Z>Jan 10, 2026</time></div><div><svg class="icon icon-tabler icon-tabler-clock" width="24" height="24" viewBox="0 0 24 24" stroke-width="2" stroke="currentColor" fill="none" stroke-linecap="round" stroke-linejoin="round"><path stroke="none" d="M0 0h24v24H0z"/><circle cx="12" cy="12" r="9"/><polyline points="12 7 12 12 15 15"/></svg>
<time class=article-time--reading>9 minute read</time></div></footer></div></header><section class=article-content><h2 id=the-problem-quick-recap>The Problem (Quick Recap)</h2><p>In our <strong><a class=link href=/p/rolling-production-p1>previous article</a></strong>, I explored a critical issue: when servers restart during deployment, in-memory timeout timers are lost, causing orders to get stuck in pending status indefinitely.</p><p><strong>The scenario</strong>: A customer places an order and the system starts a <strong>60s</strong> timer. If payment isn&rsquo;t confirmed within <strong>60s</strong>, the order should automatically timeout. But if the server restarts at <strong>second 45</strong>, that timer disappears‚Äîand the order <em>never times out</em>. The customer sees <strong>&ldquo;Order failed&rdquo;</strong> on their screen, but the backend still thinks the order is <strong>PENDING</strong>.</p><p>This creates confusion, duplicate orders, and requires manual cleanup.</p><hr><h2 id=the-core-insight>The Core Insight</h2><p>The problem isn&rsquo;t with timers themselves‚Äîit&rsquo;s where I store the timer information. When I store timeout information <strong>in the application&rsquo;s memory</strong>, it disappears when the application restarts.</p><p><strong>The solution? Move timeout tracking outside the application</strong> into a shared storage system that survives restarts.</p><h3 id=the-calendar-analogy>The Calendar Analogy</h3><p>Imagine two ways of tracking deadlines:</p><p><strong>Old Approach (In-Memory Timer)</strong><br>Each employee has a desk calendar. When they mark &ldquo;Review document by <strong>3 PM</strong>,&rdquo; only they know about it. If they go home sick, no one else can see that deadline‚Äîit&rsquo;s lost.</p><p><strong>New Approach (Shared Storage)</strong><br>The team uses a shared wall calendar in the office. When anyone marks &ldquo;Review document by <strong>3 PM</strong>,&rdquo; everyone can see it. If one person is <strong>unavailable</strong>, someone else can check the calendar and handle the task.</p><p>This is exactly what I&rsquo;m doing: moving from individual &ldquo;desk calendars&rdquo; (<strong>in-memory timers</strong>) to a &ldquo;shared wall calendar&rdquo; (<strong>Redis</strong>) that all servers can access.</p><hr><h2 id=the-solution-three-key-components>The Solution: Three Key Components</h2><p>Our solution uses three simple concepts that work together:</p><h3 id=1-the-shared-timeout-list>1. The Shared Timeout List</h3><p><strong>What it is</strong>: A shared list where I writes down every timeout and when it should happen, sorted by time.</p><p><strong>How it works</strong>: When an order is created, instead of setting a timer in the server&rsquo;s memory, I write to this shared list: &ldquo;Order #12345 should timeout at 2:30:15 PM.&rdquo;</p><p><strong>Why it survives restarts</strong>: This list lives in a database-like storage system (Redis) that&rsquo;s separate from our application servers. When a <strong>server restarts, the list is still there</strong>, untouched.</p><p><strong>Analogy</strong>: Like a shared to-do list on the wall, sorted by deadline. Everyone can see it, and it doesn&rsquo;t disappear when someone leaves the room.</p><h3 id=2-the-worker-pattern>2. The Worker Pattern</h3><p><strong>What it is</strong>: Every server regularly checks the shared timeout list to find tasks that are ready to be processed.</p><p><strong>How it works</strong>: Every second, each server asks: <em>&ldquo;Are there any timeouts in the list that have already passed?&rdquo;</em> If it finds any, it processes them.</p><p><strong>Why it&rsquo;s resilient</strong>: If <strong>one server goes down, the other servers keep checking</strong> the list. No single server is responsible for any specific timeout‚Äîthey all share the work.</p><p><strong>Analogy</strong>: Like multiple security guards on patrol. They all check the same checklist every few minutes. If one guard goes on break, the others keep checking‚Äînothing gets missed.</p><h3 id=3-the-lock-preventing-duplicates>3. The Lock (Preventing Duplicates)</h3><p><strong>What it is</strong>: A mechanism to ensure only one server processes each timeout, even though multiple servers are checking the list.</p><p><strong>How it works</strong>: When <strong>Server A</strong> sees an expired timeout, it tries to <em>&ldquo;grab&rdquo;</em> it by placing a lock. If it succeeds, it processes the timeout. If <strong>Server B</strong> tries to grab the same timeout a moment later, it <strong>sees the lock</strong> and skips it‚Äîknowing <strong>Server A</strong> is already handling it.</p><p><strong>Why it&rsquo;s necessary</strong>: Without locks, <strong>Server A and Server B might both process</strong> the same timeout, sending two &ldquo;Order timed out&rdquo; notifications to the customer.</p><p><strong>Analogy</strong>: Like task cards on a Kanban board. When you start working on a task, you move the card to <strong>&ldquo;In Progress&rdquo;</strong>.Other team members see it&rsquo;s being worked on and don&rsquo;t <em>duplicate</em> the effort.</p><hr><h2 id=how-it-all-works-together>How It All Works Together</h2><p>Let&rsquo;s walk through a complete timeout lifecycle:</p><pre class=mermaid>
  flowchart TB
    Order[Customer Places Order]
    Write[Write to Shared List:&lt;br/&gt;Order 12345 timeout at 2:30 PM]
    SharedList[(Shared Timeout List&lt;br/&gt;in Redis)]
    
    Order --&gt; Write
    Write --&gt; SharedList
    
    subgraph AllServers[All Servers Working Together]
        ServerA[Server A checks list&lt;br/&gt;every second]
        ServerB[Server B checks list&lt;br/&gt;every second]
        ServerC[Server C checks list&lt;br/&gt;every second]
    end
    
    SharedList -.-&gt;|poll| ServerA
    SharedList -.-&gt;|poll| ServerB
    SharedList -.-&gt;|poll| ServerC
    
    ServerA --&gt; FindExpired[Found expired timeout:&lt;br/&gt;Order 12345]
    FindExpired --&gt; TryLock{Try to lock&lt;br/&gt;Order 12345}
    TryLock --&gt;|Lock successful| Process[Process timeout:&lt;br/&gt;Update order status&lt;br/&gt;Notify customer]
    TryLock --&gt;|Lock failed| Skip[Skip - another&lt;br/&gt;server handling it]
    
    Process --&gt; RemoveFromList[Remove from shared list]
</pre><p><strong>Step-by-step explanation</strong>:</p><ol><li><p><strong>Order Created</strong>: Customer places Order <strong>#12345</strong>. The system writes to the shared list: &ldquo;Order #12345, <strong>timeout</strong> at <strong>2:30:15 PM</strong>&rdquo;</p></li><li><p><strong>Servers Monitor</strong>: All servers (A, B, C) independently check the shared list every second, asking <em>&ldquo;Any timeouts past their deadline?&rdquo;</em></p></li><li><p><strong>Timeout Found</strong>: At <strong>2:30:16 PM</strong>, <strong>Server A</strong> checks and finds Order <em>#12345</em> is past its deadline (by 1 second)</p></li><li><p><strong>Lock Attempt</strong>: Server A tries to <em>&ldquo;lock&rdquo;</em> Order <strong>#12345</strong>. If successful, it proceeds. If another server locked it first, Server A skips it.</p></li><li><p><strong>Process Timeout</strong>: <strong>Server A</strong> updates the order status to <strong>&ldquo;Payment Timeout&rdquo;</strong> and sends a notification to the customer</p></li><li><p><strong>Clean Up</strong>: Server A <strong>removes</strong> Order <strong>#12345</strong> from the shared list so no other server tries to process it</p></li></ol><hr><h2 id=what-happens-during-deployment>What Happens During Deployment</h2><p>This is where the new approach really shines. Let&rsquo;s compare the old and new systems during a rolling deployment:</p><h3 id=old-system-in-memory-timer>Old System: In-Memory Timer</h3><pre class=mermaid>
  sequenceDiagram
    participant Customer
    participant ServerA
    participant Kubernetes
    participant Database
    
    Customer-&gt;&gt;ServerA: Place order
    ServerA-&gt;&gt;Database: Save order (Pending)
    ServerA-&gt;&gt;ServerA: Start 60s timer&lt;br/&gt;in memory
    Note over ServerA: Timer counting: 45s...44s...43s...
    
    Kubernetes-&gt;&gt;ServerA: Deploy new version&lt;br/&gt;(restart server)
    ServerA-&gt;&gt;ServerA: Server shuts down
    Note over ServerA: ‚ùå Timer lost!
    
    Note over Database: Order stuck in Pending
    Note over Customer: App shows timeout&lt;br/&gt;but backend is inconsistent
</pre><p><strong>What goes wrong</strong>: The timer lives only in <strong>Server A&rsquo;s memory</strong>. When Server A <strong>restarts</strong>, the timer is <strong>gone</strong>. Order <em>#12345 never times ou</em>t.</p><h3 id=new-system-shared-timeout-list>New System: Shared Timeout List</h3><pre class=mermaid>
  sequenceDiagram
    participant Customer
    participant ServerA
    participant SharedList as Shared Timeout List
    participant Kubernetes
    participant ServerB
    participant Database
    
    Customer-&gt;&gt;ServerA: Place order
    ServerA-&gt;&gt;Database: Save order (Pending)
    ServerA-&gt;&gt;SharedList: Write: Order 12345&lt;br/&gt;timeout at 2:30 PM
    Note over SharedList: ‚úì Stored safely
    
    Kubernetes-&gt;&gt;ServerA: Deploy new version&lt;br/&gt;(restart server)
    ServerA-&gt;&gt;ServerA: Server shuts down
    Note over SharedList: ‚úì List still intact!
    
    ServerB-&gt;&gt;SharedList: Check for expired timeouts
    SharedList--&gt;&gt;ServerB: Order 12345 expired
    ServerB-&gt;&gt;ServerB: Lock and process timeout
    ServerB-&gt;&gt;Database: Update order (Timeout)
    ServerB-&gt;&gt;Customer: Notify: Payment timeout
    ServerB-&gt;&gt;SharedList: Remove Order 12345
</pre><p><strong>What goes right</strong>: The timeout information lives in the shared list, <em>not in Server A&rsquo;s memory</em>. When Server A restarts, Server B (still running) finds the expired timeout and processes it. The customer gets the correct notification, and the order status is updated properly.</p><hr><h2 id=trade-offs-and-considerations>Trade-offs and Considerations</h2><p>No solution is perfect. Here&rsquo;s what I gained and what I accepted:</p><h3 id=-benefits>‚úÖ Benefits</h3><p><strong>Reliability</strong>: Timeouts survive server restarts. During deployments, maintenance, or unexpected crashes, no timeout is lost.</p><p><strong>Scalability</strong>: Works across any number of servers. Adding more servers means more workers checking the list‚Äîbetter throughput.</p><p><strong>Consistency</strong>: The system state stays consistent. No more &ldquo;ghost orders&rdquo; that customers think failed but are still active in the backend.</p><h3 id=-trade-offs>‚öñÔ∏è Trade-offs</h3><p><strong>Small Processing Delay</strong>: Instead of processing timeouts at exactly 60.000 seconds, I process them within ~61 seconds (60 seconds + up to 1 second for the worker to check the list).</p><ul><li><strong>Why it&rsquo;s acceptable</strong>: For a 60-second timeout, a 1-second variance is imperceptible to customers. The business requirement is &ldquo;notify within reasonable time,&rdquo; not &ldquo;exactly at 60 seconds.&rdquo;</li></ul><p><strong>Dependency on Shared Storage</strong>: The system now depends on Redis (the shared storage) being available.</p><ul><li><strong>Why it&rsquo;s acceptable</strong>: Redis is already a critical component for many features (caching, sessions). It&rsquo;s highly reliable with built-in redundancy. If Redis goes down, I have bigger problems than timeouts.</li></ul><p><strong>Eventual Processing</strong>: There&rsquo;s a brief window where a timeout has technically expired but hasn&rsquo;t been processed yet (while waiting for the next worker check).</p><ul><li><strong>Why it&rsquo;s acceptable</strong>: The database check is idempotent‚Äîbefore processing, I verify the order is still in <strong>PENDING</strong> status. If a payment came through at the last second, I skip the timeout.</li></ul><hr><h2 id=real-world-impact>Real-World Impact</h2><p>After deploying this solution to production:</p><div class=table-wrapper><table><thead><tr><th>Metric</th><th>Before (In-Memory)</th><th>After (Shared List)</th></tr></thead><tbody><tr><td>Orders stuck during deployment</td><td>5-10 per day</td><td>0</td></tr><tr><td>Average delay for stuck orders</td><td>2-5 minutes</td><td>N/A</td></tr><tr><td>Manual cleanup required</td><td>Daily</td><td>Never</td></tr><tr><td>Customer complaints about &ldquo;ghost orders&rdquo;</td><td>Weekly</td><td>None</td></tr><tr><td>Timeout processing accuracy</td><td>Exact (but fails on restart)</td><td>Within 1 second (always works)</td></tr></tbody></table></div><p>The numbers speak for themselves. The small trade-off (1-second delay) is completely invisible to users, while the benefit (zero stuck orders) dramatically improves both user experience and operational efficiency.</p><hr><h2 id=key-takeaways>Key Takeaways</h2><p>This solution teaches us several important principles that apply beyond just timeout handling:</p><h3 id=1-persistent-state-survives-restarts>1. Persistent State Survives Restarts</h3><p>When critical information lives only in application memory, it disappears during restarts. Moving it to external, persistent storage makes it durable.</p><p><strong>Applies to</strong>: Background jobs, scheduled tasks, workflow state, temporary data that users depend on.</p><h3 id=2-shared-storage-enables-distributed-work>2. Shared Storage Enables Distributed Work</h3><p>When multiple servers can see the same work queue, they can share the load and provide redundancy. If one fails, others pick up the slack.</p><p><strong>Applies to</strong>: Task queues, job scheduling, event processing, any work that can be distributed.</p><h3 id=3-locks-prevent-duplicate-processing>3. Locks Prevent Duplicate Processing</h3><p>In distributed systems where multiple workers process the same queue, coordination (locks) ensures each item is processed exactly once.</p><p><strong>Applies to</strong>: Payment processing, notification sending, any operation that shouldn&rsquo;t happen twice.</p><h3 id=4-small-delays-are-often-acceptable>4. Small Delays Are Often Acceptable</h3><p>Perfect timing (exactly 60.000 seconds) often isn&rsquo;t necessary. &ldquo;Close enough&rdquo; (60-61 seconds) is usually fine if it makes the system more reliable.</p><p><strong>Applies to</strong>: Most user-facing features prioritize reliability over microsecond precision.</p><h3 id=5-idempotency-handles-edge-cases>5. Idempotency Handles Edge Cases</h3><p>By checking current state before acting ("<em>Is the order still pending?</em>"), the system gracefully handles race conditions and ensures consistency even if something processes twice.</p><p><strong>Applies to</strong>: Any distributed operation, retry logic, eventual consistency scenarios.</p><hr><h2 id=conclusion>Conclusion</h2><p>The journey from in-memory timers to shared timeout lists teaches a broader lesson: <strong>in distributed systems, shared persistent state is more reliable than isolated in-memory state</strong>.</p><p>While the old approach (spawn a thread, sleep, execute callback) is simpler to code, it breaks down under real-world conditions: deployments, restarts, scaling up or down. The new approach requires more infrastructure (Redis, workers, locks) but handles these conditions gracefully.</p><p>For systems running in production‚Äîespecially in containerized environments like Kubernetes where restarts are routine‚Äîthis architectural pattern is essential. Whether you&rsquo;re building e-commerce, ride-sharing, job scheduling, or any system with time-dependent logic, the same principles apply:</p><ul><li><strong>Persist critical state</strong> outside your application</li><li><strong>Distribute the work</strong> across multiple workers</li><li><strong>Coordinate with locks</strong> to prevent duplicates</li><li><strong>Accept small delays</strong> for much better reliability</li></ul><p>The result is a system that works consistently, even when individual servers come and go‚Äîwhich is exactly what modern cloud infrastructure demands.</p></section><footer class=article-footer></footer></article><aside class=related-content--wrapper><h2 class=section-title>Related content</h2><div class=related-content><div class="flex article-list--tile"><article><a href=/p/rolling-production-p1/><div class=article-details><h2 class=article-title>The Status Order Fail During Rolling Deployments - PART1</h2></div></a></article></div></div></aside><footer class=site-footer><section class=copyright>&copy;
2025 -
2026 Quang Hugo</section><section class=powerby>Built with <a href=https://gohugo.io/ target=_blank rel=noopener>Hugo</a><br>Theme <b><a href=https://github.com/CaiJimmy/hugo-theme-stack target=_blank rel=noopener data-version=3.33.0>Stack</a></b> designed by <a href=https://jimmycai.com target=_blank rel=noopener>Jimmy</a></section></footer><div class=pswp tabindex=-1 role=dialog aria-hidden=true><div class=pswp__bg></div><div class=pswp__scroll-wrap><div class=pswp__container><div class=pswp__item></div><div class=pswp__item></div><div class=pswp__item></div></div><div class="pswp__ui pswp__ui--hidden"><div class=pswp__top-bar><div class=pswp__counter></div><button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
<button class="pswp__button pswp__button--share" title=Share></button>
<button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
<button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button><div class=pswp__preloader><div class=pswp__preloader__icn><div class=pswp__preloader__cut><div class=pswp__preloader__donut></div></div></div></div></div><div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap"><div class=pswp__share-tooltip></div></div><button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
</button>
<button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)"></button><div class=pswp__caption><div class=pswp__caption__center></div></div></div></div></div><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.js integrity="sha256-ePwmChbbvXbsO02lbM3HoHbSHTHFAeChekF1xKJdleo=" crossorigin=anonymous defer></script><script src=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe-ui-default.min.js integrity="sha256-UKkzOn/w1mBxRmLLGrSeyB4e1xbrp4xylgAWb3M42pU=" crossorigin=anonymous defer></script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/default-skin/default-skin.min.css crossorigin=anonymous><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/photoswipe@4.1.3/dist/photoswipe.min.css crossorigin=anonymous></main></div><script src=https://cdn.jsdelivr.net/npm/node-vibrant@3.1.6/dist/vibrant.min.js integrity="sha256-awcR2jno4kI5X0zL8ex0vi2z+KMkF24hUW8WePSA9HM=" crossorigin=anonymous></script><script type=text/javascript src=/ts/main.c922af694cc257bf1ecc41c0dd7b0430f9114ec280ccf67cd2c6ad55f5316c4e.js defer></script><script>(function(){const e=document.createElement("link");e.href="https://fonts.googleapis.com/css2?family=Lato:wght@300;400;700&display=swap",e.type="text/css",e.rel="stylesheet",document.head.appendChild(e)})()</script><script type=module>
    import mermaid from 'https://cdn.jsdelivr.net/npm/mermaid@10/dist/mermaid.esm.min.mjs';
    
    
    const initMermaid = () => {
        const theme = document.documentElement.dataset.scheme === 'dark' ? 'dark' : 'default';
        mermaid.initialize({ 
            startOnLoad: true,
            theme: theme,
            themeVariables: {
                darkMode: theme === 'dark'
            }
        });
    };
    
    
    initMermaid();
    
    
    const observer = new MutationObserver((mutations) => {
        mutations.forEach((mutation) => {
            if (mutation.type === 'attributes' && mutation.attributeName === 'data-scheme') {
                initMermaid();
                mermaid.contentLoaded();
            }
        });
    });
    
    observer.observe(document.documentElement, {
        attributes: true,
        attributeFilter: ['data-scheme']
    });
</script></body></html>